{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1686a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK Initialized!\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "# Replace with your project ID and region\n",
    "PROJECT_ID = \"partarch-ecommerce-demo\"  \n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(\"Vertex AI SDK Initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823b12d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake.holmquist/colab-catalog-generation/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<vertexai.preview.generative_models.GenerativeModel object at 0x126b98e10>\n",
      "Of course! This is an excellent question that gets to the heart of moving from individual data science work to scalable, production-ready machine learning.\n",
      "\n",
      "The simplest way to think about it is with an analogy:\n",
      "\n",
      "A **local Jupyter Notebook** is like having a **personal kitchen**. It's fantastic for experimenting, learning to cook, and making meals for yourself. You have all your own tools (your laptop's CPU, RAM, GPU), but you're limited by the size of your kitchen and the power of your appliances.\n",
      "\n",
      "**Vertex AI** is like having access to a **fully-staffed, industrial restaurant kitchen**. It has specialized, high-powered ovens (GPUs/TPUs), massive refrigerators (scalable data storage), a streamlined process for prepping, cooking, and serving (MLOps pipelines), and a team to help you (managed services). You don't just cook there; you run a full-scale food business.\n",
      "\n",
      "---\n",
      "\n",
      "Here is a more detailed breakdown in a comparison table:\n",
      "\n",
      "| Feature / Aspect | Local Jupyter Notebook | Vertex AI (specifically Vertex AI Workbench) |\n",
      "| :--- | :--- | :--- |\n",
      "| **Scope & Purpose** | A single tool for interactive coding and experimentation. | An **end-to-end MLOps platform** for the entire ML lifecycle. |\n",
      "| **Compute & Hardware** | Limited to your local machine's CPU, RAM, and GPU. | **Virtually unlimited & on-demand.** Access powerful CPUs, massive RAM, and state-of-the-art GPUs (NVIDIA A100s, etc.) & TPUs. |\n",
      "| **Environment & Setup** | You are responsible for installing and managing everything (Python, Conda, libraries). Can lead to \"dependency hell.\" | **Managed & pre-configured.** Choose from standard Deep Learning images with all major frameworks (TensorFlow, PyTorch, Scikit-learn) already installed and optimized. |\n",
      "| **Scale & Data Handling** | Limited by your machine's RAM. Can be slow or impossible to work with terabyte-scale datasets. | **Built for large-scale data.** Natively integrates with Google Cloud Storage (GCS) and BigQuery, allowing you to process massive datasets without loading them into memory. |\n",
      "| **MLOps & Production** | **None.** It's just a notebook file. To deploy a model, you need separate tools and processes for training, versioning, and hosting. | **Core strength.** Fully integrated MLOps services: <br> • **Vertex AI Pipelines:** Automate and schedule your entire ML workflow. <br> • **Model Registry:** Version and manage your trained models. <br> • **Endpoints:** Deploy models as a scalable API with a single click. <br> • **Monitoring:** Track model performance and detect drift. |\n",
      "| **Collaboration** | Difficult. Requires external tools like Git for version control. No real-time sharing or shared resources. | **Built-in.** Uses Google Cloud's IAM for permissions. Multiple users can work on the same notebook instances, share resources, and collaborate on pipelines. |\n",
      "| **Cost** | **Free** (apart from your hardware cost). | **Pay-as-you-go.** You pay for the compute, storage, and other services you use. Can be very cost-effective since you only pay for powerful hardware when you need it. |\n",
      "| **Integration** | Isolated. Integrating with other services requires custom code and APIs. | **Deeply integrated** with the entire Google Cloud ecosystem (BigQuery, Cloud Storage, Pub/Sub, etc.). |\n",
      "\n",
      "---\n",
      "\n",
      "### Key Differences Explained in Detail\n",
      "\n",
      "#### 1. Scope: Tool vs. Platform\n",
      "\n",
      "*   A **local Jupyter Notebook** is an *application* (`.ipynb` file). It's a fantastic interactive development environment (IDE) for writing code, visualizing results, and documenting your thought process. Its scope ends when you close the application.\n",
      "*   **Vertex AI** is a *platform*. It is a suite of tools designed to manage the entire machine learning lifecycle. One of its key components is **Vertex AI Workbench**, which provides a *managed JupyterLab environment*. So, you are still using a familiar notebook interface, but it's supercharged and connected to a vast ecosystem of other services.\n",
      "\n",
      "#### 2. Hardware and Scalability\n",
      "\n",
      "*   **Local:** You have a great idea that requires a powerful GPU for deep learning. If your laptop doesn't have one, you're stuck. If your dataset grows from 1GB to 100GB, your computer with 16GB of RAM will crash.\n",
      "*   **Vertex AI:** You can start with a small, cheap virtual machine. When you need to train a large model, you can instantly provision a machine with 8 NVIDIA A100 GPUs and 256GB of RAM, run your training job, and then shut it down. You only pay for the few hours you used that powerful hardware.\n",
      "\n",
      "#### 3. MLOps: From Experiment to Product\n",
      "\n",
      "This is arguably the most significant difference.\n",
      "\n",
      "*   **Local:** You've built a great model in your notebook. Now what? To put it into an application, you have to:\n",
      "    1.  Clean up your notebook code into a Python script.\n",
      "    2.  Figure out how to save and version the model file.\n",
      "    3.  Write a web server (e.g., using Flask or FastAPI) to wrap your model.\n",
      "    4.  Rent a server (like a VM or container service).\n",
      "    5.  Deploy your web server and figure out how to scale it.\n",
      "    6.  Manually retrain and redeploy every time the data changes.\n",
      "*   **Vertex AI:** The path from notebook to production is seamless.\n",
      "    1.  Train your model in a Vertex AI Workbench notebook.\n",
      "    2.  Use the Vertex AI SDK to register the model in the **Model Registry**.\n",
      "    3.  Click a button or run a single command to deploy it to an **Endpoint**. Vertex AI handles the server, scaling, and provides you with an API to call.\n",
      "    4.  Use **Vertex AI Pipelines** to automate the entire process, so it automatically retrains and deploys a new model every week.\n",
      "\n",
      "### When to Use Which?\n",
      "\n",
      "*   **Use a Local Jupyter Notebook when you are:**\n",
      "    *   Learning Python, pandas, or machine learning fundamentals.\n",
      "    *   Working on small projects with small datasets (e.g., CSV files under a few GB).\n",
      "    *   Prototyping an idea quickly without needing cloud infrastructure.\n",
      "    *   Working in an environment with no budget or strict data privacy rules that prevent cloud usage.\n",
      "\n",
      "*   **Use Vertex AI when you are:**\n",
      "    *   Working with large datasets that don't fit on your local machine.\n",
      "    *   Needing access to powerful, specialized hardware (GPUs/TPUs) for training.\n",
      "    *   Working as part of a team that needs to collaborate.\n",
      "    *   Planning to **deploy your model into a real-world application**.\n",
      "    *   Building automated, repeatable, and robust ML systems (MLOps).\n",
      "\n",
      "In summary, you start with a local notebook to learn the craft, but you move to a platform like Vertex AI to build professional, scalable, and maintainable machine learning solutions.\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# List some available foundation models\n",
    "model = GenerativeModel(\"gemini-2.5-pro\")\n",
    "print(model)\n",
    "\n",
    "# Example: Send a prompt to Gemini\n",
    "response = model.generate_content(\"What is the difference between Vertex AI and a local Jupyter notebook?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499606f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
