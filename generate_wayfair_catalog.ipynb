{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertex AI Search for Retail: Product Catalog Generation\n",
    "\n",
    "This notebook demonstrates how to generate a synthetic product catalog for a retailer using a Vertex AI generative model (Gemini) and load it into a BigQuery table. The process is designed to be scalable and robust, handling potentially large datasets and ensuring data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet google-cloud-aiplatform pandas db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from google.cloud import bigquery, storage\n",
    "from google.colab import auth as colab_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "colab_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "Set the project-specific variables below. You can either set the environment variables or replace the `os.environ.get(...)` calls with your static values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Configuration ---\n",
    "RETAILER = \"wayfair\"\n",
    "MODEL_NAME = \"gemini-1.5-flash-001\"\n",
    "NUMBER_OF_PRODUCTS = 1000\n",
    "MAX_OUTPUT_TOKENS = 8192  # Max for gemini-1.5-flash\n",
    "MAX_PRODUCTS_PER_API_CALL = 25 # Adjust based on token usage and model capacity\n",
    "\n",
    "# --- GCP Configuration ---\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
    "GCS_BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET\")\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "# --- BigQuery Configuration ---\n",
    "BQ_DATASET_ID = \"retail\"\n",
    "BQ_TABLE_ID = f\"products-{RETAILER.lower()}\"\n",
    "\n",
    "# --- File Paths ---\n",
    "CONFIG_DIR = Path(\"config\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_FILE = OUTPUT_DIR / f\"{RETAILER}_catalog.jsonl\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "CONFIG_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"GCS Bucket: {GCS_BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration Files\n",
    "This step loads the schema, requirements, categories, and the prompt template from the `config/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_file(path):\n",
    "    \"\"\"Loads content from a file.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "try:\n",
    "    schema_str = load_config_file(CONFIG_DIR / 'schema.json')\n",
    "    requirements_str = load_config_file(CONFIG_DIR / 'requirements.txt')\n",
    "    categories_list = load_config_file(CONFIG_DIR / 'categories.txt').strip().split('\\n')\n",
    "    prompt_template = load_config_file(CONFIG_DIR / 'prompt.txt')\n",
    "    \n",
    "    # Parse the schema for BigQuery client\n",
    "    bq_schema = [bigquery.SchemaField.from_api_repr(field) for field in json.loads(schema_str)]\n",
    "    \n",
    "    print(\"Configuration files loaded successfully.\")\n",
    "    print(f\"Found {len(categories_list)} categories.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please make sure all configuration files exist in the '{CONFIG_DIR}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Product Generation\n",
    "\n",
    "This section defines the functions to generate product data using the Gemini model. It includes a function to handle individual API calls and a parallel execution framework to scale the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_parse_jsonl(raw_text):\n",
    "    \"\"\"Cleans and parses raw model output to extract valid JSONL lines.\"\"\"\n",
    "    valid_lines = []\n",
    "    for line in raw_text.strip().split('\\n'):\n",
    "        clean_line = line.strip()\n",
    "        if not clean_line:\n",
    "            continue\n",
    "        # The model sometimes wraps the output in ```jsonl ... ```, remove it.\n",
    "        if clean_line.startswith('```'):\n",
    "            continue\n",
    "        try:\n",
    "            json.loads(clean_line)  # Validate JSON\n",
    "            valid_lines.append(clean_line)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Skipping invalid JSON line: {clean_line}\")\n",
    "    return valid_lines\n",
    "\n",
    "def generate_product_batch(category, num_products):\n",
    "    \"\"\"Generates a batch of product data for a given category.\"\"\"\n",
    "    print(f\"Generating {num_products} products for category: {category}...\")\n",
    "    model = GenerativeModel(MODEL_NAME)\n",
    "    \n",
    "    prompt = prompt_template.format(\n",
    "        RETAILER=RETAILER,\n",
    "        CATEGORY=category,\n",
    "        NUMBER_OF_PRODUCTS=num_products,\n",
    "        SCHEMA=schema_str,\n",
    "        REQUIREMENTS=requirements_str\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            [prompt],\n",
    "            generation_config={\"max_output_tokens\": MAX_OUTPUT_TOKENS, \"temperature\": 0.8}\n",
    "        )\n",
    "        cleaned_lines = clean_and_parse_jsonl(response.text)\n",
    "        print(f\"Successfully generated and cleaned {len(cleaned_lines)} products for {category}.\")\n",
    "        return cleaned_lines\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating data for {category}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_catalog_in_parallel(total_products, categories):\n",
    "    \"\"\"Generates the full catalog by running batches in parallel.\"\"\"\n",
    "    all_products = []\n",
    "    products_per_category = math.ceil(total_products / len(categories))\n",
    "    \n",
    "    # Create batches of tasks\n",
    "    tasks = []\n",
    "    for category in categories:\n",
    "        remaining_for_cat = products_per_category\n",
    "        while remaining_for_cat > 0:\n",
    "            batch_size = min(remaining_for_cat, MAX_PRODUCTS_PER_API_CALL)\n",
    "            tasks.append((category, batch_size))\n",
    "            remaining_for_cat -= batch_size\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_task = {executor.submit(generate_product_batch, cat, num): (cat, num) for cat, num in tasks}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_task):\n",
    "            task = future_to_task[future]\n",
    "            try:\n",
    "                product_lines = future.result()\n",
    "                all_products.extend(product_lines)\n",
    "            except Exception as exc:\n",
    "                print(f'{task} generated an exception: {exc}')\n",
    "    \n",
    "    # Trim to the exact number of products requested\n",
    "    return all_products[:total_products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting product catalog generation...\")\n",
    "generated_products = generate_catalog_in_parallel(NUMBER_OF_PRODUCTS, categories_list)\n",
    "\n",
    "if generated_products:\n",
    "    with open(OUTPUT_FILE, 'w') as f:\n",
    "        for line in generated_products:\n",
    "            f.write(line + '\\n')\n",
    "    print(f\"\\nSuccessfully generated {len(generated_products)} products.\")\n",
    "    print(f\"Catalog saved to {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\"\\nNo products were generated. Please check the logs for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data into BigQuery\n",
    "\n",
    "This section handles uploading the generated JSONL file to Google Cloud Storage (GCS) and then loading it into a BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "    return f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "\n",
    "def load_gcs_to_bigquery(gcs_uri, dataset_id, table_id, schema):\n",
    "    \"\"\"Loads data from GCS to a BigQuery table.\"\"\"\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "    # Create dataset if it doesn't exist\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    try:\n",
    "        client.get_dataset(dataset_ref)\n",
    "        print(f\"Dataset {dataset_id} already exists.\")\n",
    "    except Exception:\n",
    "        print(f\"Creating dataset {dataset_id}...\")\n",
    "        client.create_dataset(dataset_ref)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema,\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Overwrite table if it exists\n",
    "    )\n",
    "\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    load_job = client.load_table_from_uri(gcs_uri, table_ref, job_config=job_config)\n",
    "    print(f\"Starting job {load_job.job_id} to load data into {dataset_id}.{table_id}\")\n",
    "\n",
    "    load_job.result()  # Wait for the job to complete\n",
    "    print(f\"Job finished. Loaded {load_job.output_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generated_products:\n",
    "    if not GCS_BUCKET_NAME:\n",
    "        print(\"GCS_BUCKET_NAME is not set. Skipping GCS upload and BigQuery load.\")\n",
    "    else:\n",
    "        gcs_destination_blob = f\"product_catalogs/{RETAILER}/{OUTPUT_FILE.name}\"\n",
    "        try:\n",
    "            gcs_uri = upload_to_gcs(GCS_BUCKET_NAME, OUTPUT_FILE, gcs_destination_blob)\n",
    "            load_gcs_to_bigquery(gcs_uri, BQ_DATASET_ID, BQ_TABLE_ID, bq_schema)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the GCS/BigQuery process: {e}\")\n",
    "else:\n",
    "    print(\"Skipping GCS and BigQuery steps as no products were generated.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
